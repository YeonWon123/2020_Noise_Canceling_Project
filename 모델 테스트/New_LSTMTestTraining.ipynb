{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"New_LSTMTestTraining.ipynb","provenance":[],"collapsed_sections":[],"history_visible":true,"machine_shape":"hm","authorship_tag":"ABX9TyO+miettCGIkMgYlKqT+4Ky"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"TPU"},"cells":[{"cell_type":"code","metadata":{"id":"uOj8HVOD62Wr"},"source":["# Code By adityatb at https://github.com/adityatb/noise-reduction-using-rnn\n","# LSTM method test.\n","# Maintain by ShYy, 2018.\n","\n","import scipy\n","import scipy.signal as signal\n","import numpy as np\n","import os, random, sys\n","import scipy.io.wavfile as wav\n","import math\n","import pandas as pd\n","\n","os.environ['CUDA_VISIBLE_DEVICES'] = '2'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"W-esISPz7gPt","executionInfo":{"status":"ok","timestamp":1602726491254,"user_tz":-540,"elapsed":25363,"user":{"displayName":"이명신","photoUrl":"","userId":"07889795458184672383"}},"outputId":"07aa88bc-7ac8-4088-a3e7-231588fd90ae","colab":{"base_uri":"https://localhost:8080/","height":106}},"source":["%tensorflow_version 1.8\n","import tensorflow as tf"],"execution_count":null,"outputs":[{"output_type":"stream","text":["`%tensorflow_version` only switches the major version: 1.x or 2.x.\n","You set: `1.8`. This will be interpreted as: `1.x`.\n","\n","\n","TensorFlow 1.x selected.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"k9QMZ7EtLcwl","executionInfo":{"status":"error","timestamp":1602726493522,"user_tz":-540,"elapsed":27621,"user":{"displayName":"이명신","photoUrl":"","userId":"07889795458184672383"}},"outputId":"757258f1-be14-4a0b-eb06-6d96a4f695de","colab":{"base_uri":"https://localhost:8080/","height":241}},"source":["\n","file_repository = []\n","\n","for root, _, files in os.walk(traindata):\n","    files = sorted(files)\n","\n","    #files를 읽어와 data와 samplingrate를 각각 리스트에 저장\n","    for f in files:\n","        if f.endswith(\".wav\"):\n","            srate, data = wav.read(os.path.join(root, f))\n","            file_repository.append(data)\n","            print(f)\n","\n","#training data를 pickle로 변환\n","#dataset = pd.DataFrame(columns = ['Output'])\n","#dataset.Output = file_repository\n","#dataset.to_pickle(\"/content/gdrive/My Drive/Data/dataset_output.pkl\") # pkl로 저장\n"],"execution_count":null,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-3-4a2fd4dfed98>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mfile_repository\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfiles\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwalk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraindata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mfiles\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'traindata' is not defined"]}]},{"cell_type":"code","metadata":{"id":"4wSKIIy8CgJ3","executionInfo":{"status":"ok","timestamp":1602726664230,"user_tz":-540,"elapsed":21779,"user":{"displayName":"이명신","photoUrl":"","userId":"07889795458184672383"}},"outputId":"398e9f85-283d-45c1-8bba-3f05f27b3119","colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["from google.colab import drive\n","drive.mount('/content/gdrive')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Mounted at /content/gdrive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"x94grFaxCgQ2","executionInfo":{"status":"ok","timestamp":1602726664232,"user_tz":-540,"elapsed":21225,"user":{"displayName":"이명신","photoUrl":"","userId":"07889795458184672383"}},"outputId":"06741af3-bd9d-4dad-d67f-380957db1548","colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["cd /content/gdrive/My Drive/Data/"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/content/gdrive/My Drive/Data\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"IjUjTNQRCgwx","executionInfo":{"status":"ok","timestamp":1602726664233,"user_tz":-540,"elapsed":20750,"user":{"displayName":"이명신","photoUrl":"","userId":"07889795458184672383"}},"outputId":"0cdfb863-b044-4bb3-ac70-0d8d6f909308","colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["os.getcwd()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'/content/gdrive/My Drive/Data'"]},"metadata":{"tags":[]},"execution_count":6}]},{"cell_type":"code","metadata":{"id":"uHzyuOvsMfyp"},"source":["traindata = os.getcwd() + \"/Testing/ModelOutput3/\""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"gR3KLZaV62wY"},"source":["#filename을 입력받아 앞에서부터 len(filename)-11 까지만 남긴 뒤 _voice 추가\n","def formatFilename(filename):\n","    return filename[:len(filename) - 11] + \"_voice.wav\"\n","\n","\n","#Strip away the _xnoise.wav part of the filename, and append _voice.wav to obtain clean voice counterpart\n","def create_final_sequence(sequence, max_length):\n","    a, b = sequence.shape\n","    extra_len = max_length - b\n","    null_mat = np.zeros((len(sequence), extra_len), dtype=np.float32)\n","    sequence = np.concatenate((sequence, null_mat), axis=1)\n","    return sequence\n","\n","\n","def sequentialized_spectrum(batch):\n","    # Get maximum length of batch\n","    t = []\n","    t_vec = []\n","    Sxx_Vec = []\n","    for each in batch:\n","        _, t, Sxx_Vec_Temp = signal.stft(each, fs=rate_repository[0], nperseg=stft_size, return_onesided = False) #_ 는 frequency, t는 segment time, STFT of x\n","        t_vec.append(t)   #segment time 저장\n","        Sxx_Vec.append(Sxx_Vec_Temp)    #여기에 ft된 data가 저장되는듯\n","    maximum_length = findMaxlen(t_vec)  #최대의 시간값저장?\n","\n","    max_run_total = int(math.ceil(float(maximum_length) / sequence_length))\n","    final_data = np.zeros([len(batch), max_run_total, stft_size, sequence_length])\n","    true_time = np.zeros([len(batch), max_run_total])\n","\n","    # Read in a file and compute spectrum\n","    # for batch_idx, each_set in enumerate(batch):\n","    for batch_idx, Sxx in enumerate(Sxx_Vec):\n","        # f, t, Sxx = signal.stft(each_set, fs=rate_repository[0], nperseg=stft_size, return_onesided = False)\n","\n","        # Magnitude and Phase Spectra\n","        Mag = Sxx.real\n","        t = t_vec[batch_idx]\n","        # Phase = Sxx.imag\n","\n","        # Break up the spectrum in sequence_length sized data\n","        run_full_steps = float(len(t)) / sequence_length\n","        run_total = int(math.ceil(run_full_steps))\n","\n","        # Run a loop long enough to break up all the data in the file into chunks of sequence_size\n","        for step in range(run_total):\n","\n","            begin_point = step * sequence_length\n","            end_point = begin_point + sequence_length\n","\n","            m, n = Mag[:, begin_point:end_point].shape\n","\n","            # Store each chunk sequentially in a new array, accounting for zero padding when close to the end of the file\n","            if n == sequence_length:\n","                final_data[batch_idx, step, :, :] = np.copy(Mag[:, begin_point:end_point])\n","                true_time[batch_idx, step] = n\n","            else:\n","                final_data[batch_idx, step, :, :] = np.copy(create_final_sequence(Mag[:, begin_point:end_point], sequence_length))\n","                true_time[batch_idx, step] = n\n","\n","    final_data = np.transpose(final_data, (0, 1, 3, 2))\n","\n","    return final_data, true_time, maximum_length\n","\n","#입력받은 datafile 내의 최대 len을 return\n","def findMaxlen(data_vec):\n","    max_ = 0\n","    for each in data_vec:\n","        if len(each) > max_:\n","            max_ = len(each)\n","    return max_\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1WYR-acE62y_","executionInfo":{"status":"ok","timestamp":1602727713767,"user_tz":-540,"elapsed":25423,"user":{"displayName":"이명신","photoUrl":"","userId":"07889795458184672383"}},"outputId":"108a8d02-b3cd-4c49-9548-8454b08237c3","colab":{"base_uri":"https://localhost:8080/","height":446}},"source":["# ----------------- Begin Vars --------------------- #\n","\n","# Training data directories\n","traindata = os.getcwd() + \"/Training/NoiseAdded2/\"\n","voicedata = os.getcwd() + \"/Training/HumanVoices/\"\n","checkpoints = os.getcwd() + \"/TF_Checkpoints/\"\n","\n","# NormConstant\n","norm_factor = (1 / 32768.0)\n","\n","# Spectrogram Parameters\n","stft_size = 1024\n","\n","# RNN Specs\n","sequence_length = 100\n","batch_size = 1200\n","learning_rate = 0.001\n","epochs = 250\n","# number_of_layers = 3\n","\n","# Tensorflow vars + Graph and LSTM Params\n","input_data = tf.placeholder(tf.float32, [None, sequence_length, stft_size]) #input\n","clean_data = tf.placeholder(tf.float32, [None, sequence_length, stft_size]) #label\n","sequence_length_tensor = tf.placeholder(tf.int32, (None)) \n","\n","# Temp_data_variables\n","no_of_files = 0\n","temp_list = []\n","final_data = []\n","sequence_length_id = 0\n","\n","# Repositories\n","file_repository = []\n","rate_repository = []\n","clean_repository = []\n","\n","# Selected vectors\n","files_vec = []\n","clean_files_fin_vec = []\n","clean_files_vec = []\n","\n","# Graph\n","lstm_cell = tf.contrib.rnn.BasicLSTMCell(stft_size, forget_bias = 1.0, state_is_tuple = True)\n","#lstm Cell 생성\n","\n","# stacked_lstm = tf.contrib.rnn.MultiRNNCell([[lstm_cell] for i in number_of_layers])\n","#여기선 빠른 학습을 위해 아마 층을 안쌓은것 같음\n","\n","init_state = lstm_cell.zero_state(batch_size, tf.float32)\n","#초기상태를 zero로 초기화\n","\n","rnn_outputs, final_state = tf.nn.dynamic_rnn(lstm_cell, input_data, sequence_length=sequence_length_tensor, initial_state=init_state, time_major=False)\n","#TensorFlow dynamic_rnn을 사용하면 모델의 결과와 마지막 상태값을 반환하는데, 이것들이 학습하는 동안 배치 사이에서 전달되어야 한다.\n","\n","mse_loss = tf.losses.mean_squared_error(rnn_outputs, clean_data)\n","#rnn_output을 이용 loss함수는 mse를 사용한다.\n","\n","# train_optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(mse_loss)\n","# train_optimizer = tf.train.AdagradOptimizer(learning_rate).minimize(mse_loss)\n","# train_optimizer = tf.train.AdagradDAOptimizer(learning_rate).minimize(mse_loss)\n","train_optimizer = tf.train.AdamOptimizer(learning_rate).minimize(mse_loss)\n","#optimizer로는 Adam을 사용\n","\n","saver = tf.train.Saver()\n","#tf.train.Saver()함수를 이용해 만들어놓은 cell 과 layer, optimizer, lossfunction 을 저장한다."],"execution_count":null,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:\n","The TensorFlow contrib module will not be included in TensorFlow 2.0.\n","For more information, please see:\n","  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n","  * https://github.com/tensorflow/addons\n","  * https://github.com/tensorflow/io (for I/O related ops)\n","If you depend on functionality not listed there, please file an issue.\n","\n","WARNING:tensorflow:From <ipython-input-9-90426d77e7a7>:43: BasicLSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","This class is equivalent as tf.keras.layers.LSTMCell, and will be replaced by that in Tensorflow 2.0.\n","WARNING:tensorflow:From <ipython-input-9-90426d77e7a7>:52: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n","WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/ops/rnn_cell_impl.py:735: Layer.add_variable (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Please use `layer.add_weight` method instead.\n","WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/ops/rnn_cell_impl.py:739: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Call initializer instance with the dtype argument instead of passing it to the constructor\n","WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/ops/rnn.py:244: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use tf.where in 2.0, which has the same broadcast rule as np.where\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"0wSsIots621Z"},"source":["# ------------------- Read all data to memory creating a repository of mixture and clean files --------------------- #\n","\n","os.chdir(traindata)\n","# for file_iter in range(traindata):\n","\n","# Buffer training data to memory for faster execution:\n","for root, _, files in os.walk(traindata):\n","    files = sorted(files)\n","    no_of_files = len(files)\n","    \n","\n","    #files는 NoiseAdded data들을 의미하며 배치사이즈가 training data size보다 크면 말이 안되니까 여기서 에러처리\n","    if batch_size > no_of_files:\n","        sys.exit(\"Error: batch_size cannot be more than number of files in the training directory\")\n","\n","    #files를 읽어와 data와 samplingrate를 각각 리스트에 저장\n","    for f in files:\n","        if f.endswith(\".wav\"):\n","            temp_list.append(f)\n","            srate, data = wav.read(os.path.join(root, f))\n","            file_repository.append(data)\n","            rate_repository.append(srate)\n","\n","#training data를 pickle로 변환\n","#dataset = pd.DataFrame(columns = ['Audio'])\n","#dataset.Audio = file_repository\n","#dataset.to_pickle(\"/content/gdrive/My Drive/Data/dataset.pkl\") # pkl로 저장"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"eDhleHWfz53y"},"source":["#df = pd.read_pickle(\"/content/gdrive/My Drive/Data/dataset.pkl\") #pkl 불러오기"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"p6gVw3EI0Z7q"},"source":["#df # python에서 처리하는 방법"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6o0K-dtvqBQU","executionInfo":{"status":"ok","timestamp":1602729596061,"user_tz":-540,"elapsed":1025,"user":{"displayName":"이명신","photoUrl":"","userId":"07889795458184672383"}},"outputId":"a6ab51f0-0a30-4a4c-eae6-09de66080d90","colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["# Generate a vector of file names that are clean files\n","#map은 (함수,인자) 를 입력으로 받고 들어온 모든 인자에 대해 함수를 실행시켜 리스트형태로 반환시켜주는 함수이다 고로 들어온 temp_list 인자들에 대해 formatFilename이 실행된다\n","#간단히 말하면 기존의 이름을 바꾸는 역할을 한다\n","clean_files_vec = list(map(formatFilename, temp_list))\n","# clean_files_vec = list(map(None, *clean_files_vec))\n","\n","# Find clean files that correspond to data in file_repository and buffer clean voice data to memory\n","#clean한 목소리 데이터들을 clean_repasitory에 저장\n","for root, _, files in os.walk(voicedata):\n","    files = sorted(files)\n","    for each in files:\n","        if each.endswith(\".wav\"):\n","            for name in clean_files_vec:\n","                if each == name:\n","                    srate2, data2 = wav.read(os.path.join(root, name))\n","                    clean_repository.append(data2)\n","\n","#training data를 pickle로 변환\n","#dataset2 = pd.DataFrame(columns = ['Voice'])\n","#dataset2.Voice = clean_repository\n","#dataset2.to_pickle(\"/content/gdrive/My Drive/Data/dataset_voice.pkl\") # pkl로 저장\n","\n","\"\"\"\n","#최종적으로\n","file_repository\n","rate_repository\n","clean_repository\n","가 생성됨\n","\"\"\""],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'\\n#최종적으로\\nfile_repository\\nrate_repository\\nclean_repository\\n가 생성됨\\n'"]},"metadata":{"tags":[]},"execution_count":11}]},{"cell_type":"code","metadata":{"id":"ZgVcJlzRHdoL","executionInfo":{"status":"ok","timestamp":1602729596062,"user_tz":-540,"elapsed":8,"user":{"displayName":"이명신","photoUrl":"","userId":"07889795458184672383"}},"outputId":"b599402f-3849-4e2c-e4d5-461c4e9bd69d","colab":{"base_uri":"https://localhost:8080/","height":52}},"source":["# files_vec = []\n","run_epochs = int((no_of_files / batch_size) * epochs)\n","print(run_epochs)\n","#no_of_files는 training 파일의 개수이니 3000\n","#batch_size는 10으로 설정됨\n","#epochs 는 250 이므로\n","#run_epochs 의 값은 300 * 250 = 75000\n","1 * 250"],"execution_count":null,"outputs":[{"output_type":"stream","text":["1041\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["250"]},"metadata":{"tags":[]},"execution_count":12}]},{"cell_type":"code","metadata":{"id":"b3RDG-aaClj3"},"source":["# ------------------- Step 1: Prepare data in batches and perform STFTs --------------------- #\n","# Initialize TF Graph\n","init_op = tf.global_variables_initializer()  # initialize_all_variables()\n","gpu_options = tf.GPUOptions(allow_growth = True)            #탄력적으로 GPU memory 를 할당하는 코드\n","\n","sess = tf.Session(config = tf.ConfigProto(gpu_options = gpu_options))\n","sess.run(init_op) #deep learning 실행!"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jEjusHtrXRB0","executionInfo":{"status":"error","timestamp":1602734086735,"user_tz":-540,"elapsed":24041,"user":{"displayName":"이명신","photoUrl":"","userId":"07889795458184672383"}},"outputId":"97a8c6bd-4259-4b1e-8126-b6aece264c6a","colab":{"base_uri":"https://localhost:8080/","height":389}},"source":["globalBatchLossSum = 0          # Sum of all batch losses\n","globalStepsSum = 0          # Sum of all steps\n","lastCumulativeLossAvg = 100           # Last Cumulative Loss Avg.\n","\n","#이 for문은 무슨 작업이지??\n","#run_epochs((no_of_files / batch_size) * epochs) 만큼 실행\n","for idx in range(int(1)):\n","\n","    files_vec = []\n","    #clean_files_vec = []\n","    clean_files_fin_vec = []\n","\n","    #Select batch_size no. of random number of files from file_repository and the corresponding clean files\n","    for file_iter in range(batch_size):\n","        i = random.randint(0, len(file_repository) - 1)                     #효율적인 학습을위해 file_vec에 랜덤으로 저장 \n","        files_vec.append(file_repository[i] * norm_factor)                  #file_vec 에있는   data와 clearn_files에있는 data 가 index가 일치되게 정렬되어있는가?\n","        clean_files_fin_vec.append(clean_repository[i] * norm_factor)\n","    stft_batch, sequence_length_id, maximum_length = sequentialized_spectrum(files_vec)\n","    clean_voice_batch, sequence_length_id_clean, maximum_length_clean = sequentialized_spectrum(clean_files_fin_vec)\n","\n","    # ------------------- Step 2: Feed Data to Placeholders, and then, Initialise, Train and Save the Graph  --------------------- #\n","\n","    max_time_steps = stft_batch.shape[1]\n","    batchLossSum = 0            # Sum of batch losses in one index.\n","\n","    #만들어놓은 placeholder에 data를 집어 넣는다.\n","    #왜 이걸 max_time_steps 만큼?\n","    for time_seq in range(max_time_steps):\n","        feed_dict = {\n","            input_data: stft_batch[:, time_seq, :, :],\n","            clean_data: clean_voice_batch[:, time_seq, :, :],\n","            sequence_length_tensor: sequence_length_id[:, time_seq]\n","        }\n","        _, loss_value, final_state_value, rnn_outputs_val = sess.run([train_optimizer, mse_loss, final_state, rnn_outputs], feed_dict=feed_dict)\n","\n","        # print(\"Index \" + str(idx + 1) + \" in \" + str(run_epochs))\n","        # print(\"\\tOutput Min:\\t\" + str(np.min(rnn_outputs_val)))\n","        # print(\"\\tClean Min:\\t\" + str(np.min(clean_voice_batch[:, time_seq, :, :])))\n","        # print(\"\\tOutput Max:\\t\" + str(np.max(rnn_outputs_val)))\n","        # print(\"\\tClean Max:\\t\" + str(np.max(clean_voice_batch[:, time_seq, :, :])))\n","        # print(\"\\tBatch Loss:\\t\" + str(loss_value * 32768))          # Multiplied 32768 to show the batch losses obviously.\n","        batchLossSum = batchLossSum + loss_value\n","\n","    print(\"\\t\\tIndex \" + str(idx + 1) + \" Batch Loss Avg:\\t\" + str(batchLossSum / max_time_steps / norm_factor) + \"\\n\")\n","\n","    globalBatchLossSum = globalBatchLossSum + batchLossSum\n","    globalStepsSum = globalStepsSum + max_time_steps\n","\n","    #if (int((idx + 1) % no_of_files) == 0):\n","    if (int((idx + 1) % 500) == 0):\n","        # All batch losses sum divide global steps to get Avg\n","        cumulativLossAvg = globalBatchLossSum / globalStepsSum\n","        print(\"\\n\\t\\tCumulative epochs loss Avg in latest \" + str(idx + 1) + \" indexes:\\t\" + str(cumulativLossAvg / norm_factor))\n","        if(cumulativLossAvg <= lastCumulativeLossAvg):\n","            lastCumulativeLossAvg = cumulativLossAvg            # If cumulative loss avg is smaller or equal to last avg, stay learning rate\n","        else:\n","            learning_rate = learning_rate / 5           # If cumulative loss avg is bigger than last avg, than change learning rate to 1/5\n","            lastCumulativeLossAvg = cumulativLossAvg\n","            print(\"\\n\\t\\tLearning Rate changed to: \" + str(learning_rate))\n","        globalBatchLossSum = 0          # Initialize to 0, for next indexes batch loss calculation\n","        globalStepsSum = 0\n","\n","        os.chdir(checkpoints)\n","        saver.save(sess, './ssep_model_increaseBatchSize.ckpt', global_step=idx)\n","        print(\"\\t\\tSaved checkpoint\\n\")\n","        os.chdir(traindata)\n","\n","os.chdir(checkpoints)\n","saver.save(sess, './FINAL.ckpt')\n","print(\"Saved FINAL\")\n","sess.close()"],"execution_count":22,"outputs":[{"output_type":"error","ename":"RuntimeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-22-94355a673f54>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     32\u001b[0m             \u001b[0msequence_length_tensor\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0msequence_length_id\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtime_seq\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         }\n\u001b[0;32m---> 34\u001b[0;31m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinal_state_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrnn_outputs_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_optimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmse_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinal_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrnn_outputs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0;31m# print(\"Index \" + str(idx + 1) + \" in \" + str(run_epochs))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tensorflow-1.15.2/python3.6/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    954\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    955\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 956\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    957\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    958\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tensorflow-1.15.2/python3.6/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1101\u001b[0m     \u001b[0;31m# Check session.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_closed\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1103\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Attempted to use a closed Session.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1104\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mversion\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1105\u001b[0m       raise RuntimeError('The Session graph is empty.  Add operations to the '\n","\u001b[0;31mRuntimeError\u001b[0m: Attempted to use a closed Session."]}]},{"cell_type":"code","metadata":{"id":"JF8IVy57X3oy","executionInfo":{"status":"ok","timestamp":1602733822941,"user_tz":-540,"elapsed":738,"user":{"displayName":"이명신","photoUrl":"","userId":"07889795458184672383"}}},"source":["max_time_steps = stft_batch.shape[1]"],"execution_count":19,"outputs":[]},{"cell_type":"code","metadata":{"id":"w0JEyEwiYV-a","executionInfo":{"status":"ok","timestamp":1602733829732,"user_tz":-540,"elapsed":605,"user":{"displayName":"이명신","photoUrl":"","userId":"07889795458184672383"}},"outputId":"d928477f-e045-4321-cb38-c064def6d79a","colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["max_time_steps"],"execution_count":20,"outputs":[{"output_type":"execute_result","data":{"text/plain":["4"]},"metadata":{"tags":[]},"execution_count":20}]},{"cell_type":"code","metadata":{"id":"ywuGr1tdYCsS","executionInfo":{"status":"ok","timestamp":1602733817325,"user_tz":-540,"elapsed":584,"user":{"displayName":"이명신","photoUrl":"","userId":"07889795458184672383"}},"outputId":"2d9887ba-f7b8-4e92-973d-e3cb6acd2eb3","colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["stft_batch.shape"],"execution_count":18,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(1200, 4, 100, 1024)"]},"metadata":{"tags":[]},"execution_count":18}]},{"cell_type":"code","metadata":{"id":"NeDbtkEX6mJj","executionInfo":{"status":"error","timestamp":1602733531443,"user_tz":-540,"elapsed":16940,"user":{"displayName":"이명신","photoUrl":"","userId":"07889795458184672383"}},"outputId":"1720f378-e94a-4141-d9dd-41a545f2e854","colab":{"base_uri":"https://localhost:8080/","height":365}},"source":["globalBatchLossSum = 0          # Sum of all batch losses\n","globalStepsSum = 0          # Sum of all steps\n","lastCumulativeLossAvg = 100           # Last Cumulative Loss Avg.\n","\n","#이 for문은 무슨 작업이지??\n","#run_epochs((no_of_files / batch_size) * epochs) 만큼 실행\n","for idx in range(int(run_epochs)):\n","\n","    files_vec = []\n","    #clean_files_vec = []\n","    clean_files_fin_vec = []\n","\n","    #Select batch_size no. of random number of files from file_repository and the corresponding clean files\n","    for file_iter in range(batch_size):\n","        i = random.randint(0, len(file_repository) - 1)                     #효율적인 학습을위해 file_vec에 랜덤으로 저장 \n","        files_vec.append(file_repository[i] * norm_factor)                  #file_vec 에있는   data와 clearn_files에있는 data 가 index가 일치되게 정렬되어있는가?\n","        clean_files_fin_vec.append(clean_repository[i] * norm_factor)\n","    stft_batch, sequence_length_id, maximum_length = sequentialized_spectrum(files_vec)\n","    clean_voice_batch, sequence_length_id_clean, maximum_length_clean = sequentialized_spectrum(clean_files_fin_vec)\n","\n","    # ------------------- Step 2: Feed Data to Placeholders, and then, Initialise, Train and Save the Graph  --------------------- #\n","\n","    max_time_steps = stft_batch.shape[1]\n","    batchLossSum = 0            # Sum of batch losses in one index.\n","\n","    #만들어놓은 placeholder에 data를 집어 넣는다.\n","    #왜 이걸 max_time_steps 만큼?\n","    for time_seq in range(max_time_steps):\n","        feed_dict = {\n","            input_data: stft_batch[:, time_seq, :, :],\n","            clean_data: clean_voice_batch[:, time_seq, :, :],\n","            sequence_length_tensor: sequence_length_id[:, time_seq]\n","        }\n","        _, loss_value, final_state_value, rnn_outputs_val = sess.run([train_optimizer, mse_loss, final_state, rnn_outputs], feed_dict=feed_dict)\n","\n","        # print(\"Index \" + str(idx + 1) + \" in \" + str(run_epochs))\n","        # print(\"\\tOutput Min:\\t\" + str(np.min(rnn_outputs_val)))\n","        # print(\"\\tClean Min:\\t\" + str(np.min(clean_voice_batch[:, time_seq, :, :])))\n","        # print(\"\\tOutput Max:\\t\" + str(np.max(rnn_outputs_val)))\n","        # print(\"\\tClean Max:\\t\" + str(np.max(clean_voice_batch[:, time_seq, :, :])))\n","        # print(\"\\tBatch Loss:\\t\" + str(loss_value * 32768))          # Multiplied 32768 to show the batch losses obviously.\n","        batchLossSum = batchLossSum + loss_value\n","\n","    print(\"\\t\\tIndex \" + str(idx + 1) + \" Batch Loss Avg:\\t\" + str(batchLossSum / max_time_steps / norm_factor) + \"\\n\")\n","\n","    globalBatchLossSum = globalBatchLossSum + batchLossSum\n","    globalStepsSum = globalStepsSum + max_time_steps\n","\n","    #if (int((idx + 1) % no_of_files) == 0):\n","    if (int((idx + 1) % 500) == 0):\n","        # All batch losses sum divide global steps to get Avg\n","        cumulativLossAvg = globalBatchLossSum / globalStepsSum\n","        print(\"\\n\\t\\tCumulative epochs loss Avg in latest \" + str(idx + 1) + \" indexes:\\t\" + str(cumulativLossAvg / norm_factor))\n","        if(cumulativLossAvg <= lastCumulativeLossAvg):\n","            lastCumulativeLossAvg = cumulativLossAvg            # If cumulative loss avg is smaller or equal to last avg, stay learning rate\n","        else:\n","            learning_rate = learning_rate / 5           # If cumulative loss avg is bigger than last avg, than change learning rate to 1/5\n","            lastCumulativeLossAvg = cumulativLossAvg\n","            print(\"\\n\\t\\tLearning Rate changed to: \" + str(learning_rate))\n","        globalBatchLossSum = 0          # Initialize to 0, for next indexes batch loss calculation\n","        globalStepsSum = 0\n","\n","        os.chdir(checkpoints)\n","        saver.save(sess, './ssep_model_increaseBatchSize.ckpt', global_step=idx)\n","        print(\"\\t\\tSaved checkpoint\\n\")\n","        os.chdir(traindata)\n","\n","os.chdir(checkpoints)\n","saver.save(sess, './FINAL.ckpt')\n","print(\"Saved FINAL\")\n","sess.close()"],"execution_count":14,"outputs":[{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-14-6dc22961d706>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mfiles_vec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_repository\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnorm_factor\u001b[0m\u001b[0;34m)\u001b[0m                  \u001b[0;31m#file_vec 에있는   data와 clearn_files에있는 data 가 index가 일치되게 정렬되어있는가?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mclean_files_fin_vec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclean_repository\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnorm_factor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0mstft_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msequence_length_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaximum_length\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msequentialized_spectrum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiles_vec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m     \u001b[0mclean_voice_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msequence_length_id_clean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaximum_length_clean\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msequentialized_spectrum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclean_files_fin_vec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-8-42c98cfdbf4d>\u001b[0m in \u001b[0;36msequentialized_spectrum\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0mmax_run_total\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mceil\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaximum_length\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0msequence_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m     \u001b[0mfinal_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_run_total\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstft_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msequence_length\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m     \u001b[0mtrue_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_run_total\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]}]}