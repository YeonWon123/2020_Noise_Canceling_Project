{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"New_LSTMTestTraining_AddedLayer.ipynb","provenance":[],"collapsed_sections":[],"machine_shape":"hm","authorship_tag":"ABX9TyPQgbyXO4y3jKEJN3EESiI0"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"TPU"},"cells":[{"cell_type":"code","metadata":{"id":"uOj8HVOD62Wr","executionInfo":{"status":"ok","timestamp":1601872358563,"user_tz":-540,"elapsed":975,"user":{"displayName":"이명신","photoUrl":"","userId":"07889795458184672383"}}},"source":["# Code By adityatb at https://github.com/adityatb/noise-reduction-using-rnn\n","# LSTM method test.\n","# Maintain by ShYy, 2018.\n","\n","import scipy\n","import scipy.signal as signal\n","import numpy as np\n","import os, random, sys\n","import scipy.io.wavfile as wav\n","import math\n","\n","os.environ['CUDA_VISIBLE_DEVICES'] = '2'"],"execution_count":1,"outputs":[]},{"cell_type":"code","metadata":{"id":"W-esISPz7gPt","executionInfo":{"status":"ok","timestamp":1601872380979,"user_tz":-540,"elapsed":23378,"user":{"displayName":"이명신","photoUrl":"","userId":"07889795458184672383"}},"outputId":"1012174b-a006-40dd-b731-ef6929d54bb0","colab":{"base_uri":"https://localhost:8080/","height":105}},"source":["%tensorflow_version 1.8\n","import tensorflow as tf"],"execution_count":2,"outputs":[{"output_type":"stream","text":["`%tensorflow_version` only switches the major version: 1.x or 2.x.\n","You set: `1.8`. This will be interpreted as: `1.x`.\n","\n","\n","TensorFlow 1.x selected.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"4wSKIIy8CgJ3","executionInfo":{"status":"ok","timestamp":1601872404131,"user_tz":-540,"elapsed":46520,"user":{"displayName":"이명신","photoUrl":"","userId":"07889795458184672383"}},"outputId":"8cea132c-70d3-40b0-917e-5c0e0a6bda6e","colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["from google.colab import drive\n","drive.mount('/content/gdrive')"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Mounted at /content/gdrive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"x94grFaxCgQ2","executionInfo":{"status":"ok","timestamp":1601872404132,"user_tz":-540,"elapsed":46512,"user":{"displayName":"이명신","photoUrl":"","userId":"07889795458184672383"}},"outputId":"fe741231-22b3-40e8-d72c-ab9e8033b497","colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["cd /content/gdrive/My Drive/Data/"],"execution_count":4,"outputs":[{"output_type":"stream","text":["/content/gdrive/My Drive/Data\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"IjUjTNQRCgwx","executionInfo":{"status":"ok","timestamp":1601872404134,"user_tz":-540,"elapsed":46505,"user":{"displayName":"이명신","photoUrl":"","userId":"07889795458184672383"}},"outputId":"43299303-58f2-4211-d8ba-a44473a2f3ca","colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["os.getcwd()"],"execution_count":5,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'/content/gdrive/My Drive/Data'"]},"metadata":{"tags":[]},"execution_count":5}]},{"cell_type":"code","metadata":{"id":"gR3KLZaV62wY","executionInfo":{"status":"ok","timestamp":1601872404135,"user_tz":-540,"elapsed":46494,"user":{"displayName":"이명신","photoUrl":"","userId":"07889795458184672383"}}},"source":["#filename을 입력받아 앞에서부터 len(filename)-11 까지만 남긴 뒤 _voice 추가\n","def formatFilename(filename):\n","    return filename[:len(filename) - 11] + \"_voice.wav\"\n","\n","\n","# Strip away the _xnoise.wav part of the filename, and append _voice.wav to obtain clean voice counterpart\n","\n","def create_final_sequence(sequence, max_length):\n","    a, b = sequence.shape\n","    extra_len = max_length - b\n","    null_mat = np.zeros((len(sequence), extra_len), dtype=np.float32)\n","    sequence = np.concatenate((sequence, null_mat), axis=1)\n","    return sequence\n","\n","\n","def sequentialized_spectrum(batch):\n","    # Get maximum length of batch\n","    t = []\n","    t_vec = []\n","    Sxx_Vec = []\n","    for each in batch:\n","        _, t, Sxx_Vec_Temp = signal.stft(each, fs=rate_repository[0], nperseg=stft_size, return_onesided = False)\n","        t_vec.append(t)\n","        Sxx_Vec.append(Sxx_Vec_Temp)\n","    maximum_length = findMaxlen(t_vec)\n","\n","    max_run_total = int(math.ceil(float(maximum_length) / sequence_length))\n","    final_data = np.zeros([len(batch), max_run_total, stft_size, sequence_length])\n","    true_time = np.zeros([len(batch), max_run_total])\n","\n","    # Read in a file and compute spectrum\n","    # for batch_idx, each_set in enumerate(batch):\n","    for batch_idx, Sxx in enumerate(Sxx_Vec):\n","        # f, t, Sxx = signal.stft(each_set, fs=rate_repository[0], nperseg=stft_size, return_onesided = False)\n","\n","        # Magnitude and Phase Spectra\n","        Mag = Sxx.real\n","        t = t_vec[batch_idx]\n","        # Phase = Sxx.imag\n","\n","        # Break up the spectrum in sequence_length sized data\n","        run_full_steps = float(len(t)) / sequence_length\n","        run_total = int(math.ceil(run_full_steps))\n","\n","        # Run a loop long enough to break up all the data in the file into chunks of sequence_size\n","        for step in range(run_total):\n","\n","            begin_point = step * sequence_length\n","            end_point = begin_point + sequence_length\n","\n","            m, n = Mag[:, begin_point:end_point].shape\n","\n","            # Store each chunk sequentially in a new array, accounting for zero padding when close to the end of the file\n","            if n == sequence_length:\n","                final_data[batch_idx, step, :, :] = np.copy(Mag[:, begin_point:end_point])\n","                true_time[batch_idx, step] = n\n","            else:\n","                final_data[batch_idx, step, :, :] = np.copy(create_final_sequence(Mag[:, begin_point:end_point], sequence_length))\n","                true_time[batch_idx, step] = n\n","\n","    final_data = np.transpose(final_data, (0, 1, 3, 2))\n","\n","    return final_data, true_time, maximum_length\n","\n","\n","def findMaxlen(data_vec):\n","    max_ = 0\n","    for each in data_vec:\n","        if len(each) > max_:\n","            max_ = len(each)\n","    return max_\n","\n"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"id":"1WYR-acE62y_","executionInfo":{"status":"error","timestamp":1601872674586,"user_tz":-540,"elapsed":774,"user":{"displayName":"이명신","photoUrl":"","userId":"07889795458184672383"}},"outputId":"aca60814-19a1-494c-efdc-23393a7ce51f","colab":{"base_uri":"https://localhost:8080/","height":422}},"source":["# ----------------- Begin Vars --------------------- #\n","\n","# Training data directories\n","traindata = os.getcwd() + \"/Training/Self_NoiseAdded/\"\n","voicedata = os.getcwd() + \"/Training/HumanVoices2/\"\n","checkpoints = os.getcwd() + \"/TF_Checkpoints2/\"\n","\n","# NormConstant\n","norm_factor = (1 / 32768.0)\n","\n","# Spectrogram Parameters\n","stft_size = 1024\n","\n","# RNN Specs\n","sequence_length = 100\n","batch_size = 10\n","learning_rate = 0.001\n","epochs = 250\n","number_of_layers = 3\n","\n","# Tensorflow vars + Graph and LSTM Params\n","input_data = tf.placeholder(tf.float32, [None, sequence_length, stft_size])\n","clean_data = tf.placeholder(tf.float32, [None, sequence_length, stft_size])\n","sequence_length_tensor = tf.placeholder(tf.int32, (None))\n","\n","# Temp_data_variables\n","no_of_files = 0\n","temp_list = []\n","final_data = []\n","sequence_length_id = 0\n","\n","# Repositories\n","file_repository = []\n","rate_repository = []\n","clean_repository = []\n","\n","# Selected vectors\n","files_vec = []\n","clean_files_fin_vec = []\n","clean_files_vec = []\n","\n","# Graph\n","lstm_cell = tf.contrib.rnn.BasicLSTMCell(stft_size, forget_bias = 1.0)\n","lstm_cell2 = tf.contrib.rnn.BasicLSTMCell(stft_size, forget_bias = 1.0, state_is_tuple = True)\n","lstm_cell3 = tf.contrib.rnn.BasicLSTMCell(stft_size, forget_bias = 1.0, state_is_tuple = True)\n","#lstm Cell 생성\n","\n","#stacked_lstm = tf.contrib.rnn.MultiRNNCell([[lstm_cell] for i in range(number_of_layers)])\n","#stacked_lstm = tf.contrib.rnn.MultiRNNCell([lstm_cell,lstm_cell2])\n","#stacked_lstm = tf.contrib.rnn.MultiRNNCell([lstm_cell],state_is_tuple=True)\n","#층 쌓기\n","\n","init_state = lstm_cell.zero_state(batch_size, tf.float32)\n","#초기상태를 zero로 초기화\n","\n","rnn_outputs, final_state = tf.nn.dynamic_rnn(lstm_cell, input_data, sequence_length=sequence_length_tensor, initial_state=init_state, time_major=False)\n","#rnn_outputs, final_state = tf.nn.static_rnn(stacked_lstm, input_data, sequence_length=sequence_length_tensor, initial_state=init_state)\n","#TensorFlow dynamic_rnn을 사용하면 모델의 결과와 마지막 상태값을 반환하는데, 이것들이 학습하는 동안 배치 사이에서 전달되어야 한다.\n","\n","mse_loss = tf.losses.mean_squared_error(rnn_outputs, clean_data)\n","#rnn_output을 이용 loss함수는 mse를 사용한다.\n","\n","# train_optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(mse_loss)\n","# train_optimizer = tf.train.AdagradOptimizer(learning_rate).minimize(mse_loss)\n","# train_optimizer = tf.train.AdagradDAOptimizer(learning_rate).minimize(mse_loss)\n","train_optimizer = tf.train.AdamOptimizer(learning_rate).minimize(mse_loss)\n","#optimizer로는 Adam을 사용\n","\n","saver = tf.train.Saver()\n","#tf.train.Saver()함수를 이용해 만들어놓은 cell 과 layer, optimizer, lossfunction 을 저장한다."],"execution_count":9,"outputs":[{"output_type":"error","ename":"TypeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-9-99d13f1f03f6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;31m#초기상태를 zero로 초기화\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m \u001b[0mrnn_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinal_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdynamic_rnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstacked_lstm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msequence_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msequence_length_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitial_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minit_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtime_major\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m \u001b[0;31m#rnn_outputs, final_state = tf.nn.static_rnn(stacked_lstm, input_data, sequence_length=sequence_length_tensor, initial_state=init_state)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;31m#TensorFlow dynamic_rnn을 사용하면 모델의 결과와 마지막 상태값을 반환하는데, 이것들이 학습하는 동안 배치 사이에서 전달되어야 한다.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tensorflow-1.15.2/python3.6/tensorflow_core/python/util/deprecation.py\u001b[0m in \u001b[0;36mnew_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    322\u001b[0m               \u001b[0;34m'in a future version'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mdate\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'after %s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mdate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m               instructions)\n\u001b[0;32m--> 324\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    325\u001b[0m     return tf_decorator.make_decorator(\n\u001b[1;32m    326\u001b[0m         \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'deprecated'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tensorflow-1.15.2/python3.6/tensorflow_core/python/ops/rnn.py\u001b[0m in \u001b[0;36mdynamic_rnn\u001b[0;34m(cell, inputs, sequence_length, initial_state, dtype, parallel_iterations, swap_memory, time_major, scope)\u001b[0m\n\u001b[1;32m    636\u001b[0m     \u001b[0mValueError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIf\u001b[0m \u001b[0minputs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0man\u001b[0m \u001b[0mempty\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    637\u001b[0m   \"\"\"\n\u001b[0;32m--> 638\u001b[0;31m   \u001b[0mrnn_cell_impl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massert_like_rnncell\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cell\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    639\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    640\u001b[0m   \u001b[0;32mwith\u001b[0m \u001b[0mvs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvariable_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscope\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m\"rnn\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mvarscope\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tensorflow-1.15.2/python3.6/tensorflow_core/python/ops/rnn_cell_impl.py\u001b[0m in \u001b[0;36massert_like_rnncell\u001b[0;34m(cell_name, cell)\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[0merrors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0merror\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcond\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconditions\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcond\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m     raise TypeError(\"The argument {!r} ({}) is not an RNNCell: {}.\".format(\n\u001b[0;32m--> 102\u001b[0;31m         cell_name, cell, \", \".join(errors)))\n\u001b[0m\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mTypeError\u001b[0m: The argument 'cell' (<tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x7fbb9f4dc668>) is not an RNNCell: 'output_size' property is missing, 'state_size' property is missing."]}]},{"cell_type":"code","metadata":{"id":"0wSsIots621Z","executionInfo":{"status":"aborted","timestamp":1601872406142,"user_tz":-540,"elapsed":48484,"user":{"displayName":"이명신","photoUrl":"","userId":"07889795458184672383"}}},"source":["# ------------------- Read all data to memory creating a repository of mixture and clean files --------------------- #\n","\n","os.chdir(traindata)\n","# for file_iter in range(traindata):\n","\n","# Buffer training data to memory for faster execution:\n","for root, _, files in os.walk(traindata):\n","    files = sorted(files)\n","    no_of_files = len(files)\n","\n","    #files는 NoiseAdded data들을 의미하며 배치사이즈가 training data size보다 크면 말이 안되니까 여기서 에러처리\n","    if batch_size > no_of_files:\n","        sys.exit(\"Error: batch_size cannot be more than number of files in the training directory\")\n","\n","    #files를 읽어와 data와 samplingrate를 각각 리스트에 저장\n","    for f in files:\n","        if f.endswith(\".wav\"):\n","            temp_list.append(f)\n","            srate, data = wav.read(os.path.join(root, f))\n","            file_repository.append(data)\n","            rate_repository.append(srate)\n","\n","# Generate a vector of file names that are clean files\n","#map은 (함수,인자) 를 입력으로 받고 들어온 모든 인자에 대해 함수를 실행시켜 리스트형태로 반환시켜주는 함수이다 고로 들어온 temp_list 인자들에 대해 formatFilename이 실행된다\n","#간단히 말하면 기존의 이름을 바꾸는 역할을 한다\n","clean_files_vec = list(map(formatFilename, temp_list))\n","# clean_files_vec = list(map(None, *clean_files_vec))\n","\n","# Find clean files that correspond to data in file_repository and buffer clean voice data to memory\n","#clean한 목소리 데이터들을 clean_repasitory에 저장\n","for root, _, files in os.walk(voicedata):\n","    files = sorted(files)\n","    for each in files:\n","        if each.endswith(\".wav\"):\n","            for name in clean_files_vec:\n","                if each == name:\n","                    srate2, data2 = wav.read(os.path.join(root, name))\n","                    clean_repository.append(data2)\n","\n","\"\"\"\n","#최종적으로\n","file_repository\n","rate_repository\n","clean_repository\n","가 생성됨\n","\"\"\""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZgVcJlzRHdoL","executionInfo":{"status":"aborted","timestamp":1601872406145,"user_tz":-540,"elapsed":48477,"user":{"displayName":"이명신","photoUrl":"","userId":"07889795458184672383"}}},"source":["# files_vec = []\n","run_epochs = int((no_of_files / batch_size) * epochs)\n","print(run_epochs)\n","#no_of_files는 training 파일의 개수이니 3000\n","#batch_size는 10으로 설정됨\n","#epochs 는 250 이므로\n","#run_epochs 의 값은 300 * 250 = 75000"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NeDbtkEX6mJj","executionInfo":{"status":"aborted","timestamp":1601872406147,"user_tz":-540,"elapsed":48470,"user":{"displayName":"이명신","photoUrl":"","userId":"07889795458184672383"}}},"source":["# ------------------- Step 1: Prepare data in batches and perform STFTs --------------------- #\n","# Initialize TF Graph\n","init_op = tf.global_variables_initializer()  # initialize_all_variables()\n","gpu_options = tf.GPUOptions(allow_growth = True)            # Set session GPU using growing.\n","\n","sess = tf.Session(config = tf.ConfigProto(gpu_options = gpu_options))\n","sess.run(init_op)\n","\n","globalBatchLossSum = 0          # Sum of all batch losses\n","globalStepsSum = 0          # Sum of all steps\n","lastCumulativeLossAvg = 100           # Last Cumulative Loss Avg.\n","\n","for idx in range(int(run_epochs)):\n","\n","    files_vec = []\n","    # clean_files_vec = []\n","    clean_files_fin_vec = []\n","\n","    # Select batch_size no. of random number of files from file_repository and the corresponding clean files\n","    for file_iter in range(batch_size):\n","        i = random.randint(0, len(file_repository) - 1)\n","        files_vec.append(file_repository[i] * norm_factor)\n","        clean_files_fin_vec.append(clean_repository[i] * norm_factor)\n","\n","    stft_batch, sequence_length_id, maximum_length = sequentialized_spectrum(files_vec)\n","    clean_voice_batch, sequence_length_id_clean, maximum_length_clean = sequentialized_spectrum(clean_files_fin_vec)\n","\n","    # ------------------- Step 2: Feed Data to Placeholders, and then, Initialise, Train and Save the Graph  --------------------- #\n","\n","    max_time_steps = stft_batch.shape[1]\n","    batchLossSum = 0            # Sum of batch losses in one index.\n","\n","    for time_seq in range(max_time_steps):\n","        feed_dict = {\n","            input_data: stft_batch[:, time_seq, :, :],\n","            clean_data: clean_voice_batch[:, time_seq, :, :],\n","            sequence_length_tensor: sequence_length_id[:, time_seq]\n","        }\n","        _, loss_value, final_state_value, rnn_outputs_val = sess.run([train_optimizer, mse_loss, final_state, rnn_outputs], feed_dict=feed_dict)\n","\n","        # print(\"Index \" + str(idx + 1) + \" in \" + str(run_epochs))\n","        # print(\"\\tOutput Min:\\t\" + str(np.min(rnn_outputs_val)))\n","        # print(\"\\tClean Min:\\t\" + str(np.min(clean_voice_batch[:, time_seq, :, :])))\n","        # print(\"\\tOutput Max:\\t\" + str(np.max(rnn_outputs_val)))\n","        # print(\"\\tClean Max:\\t\" + str(np.max(clean_voice_batch[:, time_seq, :, :])))\n","        # print(\"\\tBatch Loss:\\t\" + str(loss_value * 32768))          # Multiplied 32768 to show the batch losses obviously.\n","        batchLossSum = batchLossSum + loss_value\n","\n","    print(\"\\t\\tIndex \" + str(idx + 1) + \" Batch Loss Avg:\\t\" + str(batchLossSum / max_time_steps / norm_factor) + \"\\n\")\n","\n","    globalBatchLossSum = globalBatchLossSum + batchLossSum\n","    globalStepsSum = globalStepsSum + max_time_steps\n","\n","    if (int((idx + 1) % no_of_files) == 0):\n","        # All batch losses sum divide global steps to get Avg\n","        cumulativLossAvg = globalBatchLossSum / globalStepsSum\n","        print(\"\\n\\t\\tCumulative epochs loss Avg in latest \" + str(idx + 1) + \" indexes:\\t\" + str(cumulativLossAvg / norm_factor))\n","        if(cumulativLossAvg <= lastCumulativeLossAvg):\n","            lastCumulativeLossAvg = cumulativLossAvg            # If cumulative loss avg is smaller or equal to last avg, stay learning rate\n","        else:\n","            learning_rate = learning_rate / 5           # If cumulative loss avg is bigger than last avg, than change learning rate to 1/5\n","            lastCumulativeLossAvg = cumulativLossAvg\n","            print(\"\\n\\t\\tLearning Rate changed to: \" + str(learning_rate))\n","        globalBatchLossSum = 0          # Initialize to 0, for next indexes batch loss calculation\n","        globalStepsSum = 0\n","\n","        os.chdir(checkpoints)\n","        saver.save(sess, './ssep_model_AddedLayer.ckpt', global_step=idx)\n","        print(\"\\t\\tSaved checkpoint\\n\")\n","        os.chdir(traindata)\n","\n","os.chdir(checkpoints)\n","saver.save(sess, './FINAL.ckpt')\n","print(\"Saved FINAL\")\n","sess.close()"],"execution_count":null,"outputs":[]}]}